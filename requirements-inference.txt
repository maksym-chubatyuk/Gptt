# Requirements for inference (main.py)
#
# Install:
#   pip install -r requirements-inference.txt
#
# Also build llama.cpp server:
#   git clone https://github.com/ggerganov/llama.cpp
#   cd llama.cpp
#   cmake -B build -DGGML_METAL=on   # Mac
#   cmake -B build -DGGML_CUDA=on    # Linux with NVIDIA
#   cmake --build build --target llama-server -j

# Camera capture
opencv-python>=4.8.0

# Image processing
Pillow>=10.0.0

# HTTP client for llama.cpp server API
requests>=2.28.0
